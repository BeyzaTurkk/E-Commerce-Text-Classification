# -*- coding: utf-8 -*-
"""Untitled1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1MUSiuJzuV0pwSxFDvhaQ4tod6-qqIGTy

# Yeni Bölüm
"""

from sklearn.model_selection import GridSearchCV, RandomizedSearchCV

def hyperparameter_tuning(model, param_grid, X_train, y_train, search_type="grid", cv=5, n_iter=10, scoring="accuracy"):


    if search_type == "grid":
        search = GridSearchCV(model, param_grid, cv=cv, scoring=scoring, n_jobs=-1, verbose=1)
    elif search_type == "random":
        search = RandomizedSearchCV(model, param_grid, n_iter=n_iter, cv=cv, scoring=scoring, n_jobs=-1, verbose=1, random_state=42)
    else:
        raise ValueError("Invalid search_type. Choose 'grid' or 'random'.")

    search.fit(X_train, y_train)
    results_df = pd.DataFrame(search.cv_results_)
    return search.best_estimator_, search.best_params_, search.best_score_,results_df

import pandas as pd
import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk.stem import WordNetLemmatizer
import string
from sklearn.feature_extraction.text import TfidfVectorizer


nltk.download("punkt_tab")
nltk.download("stopwords")
nltk.download("wordnet")


df = pd.read_csv("ecommerceDataset.csv", header=None, names=["Category", "Description"])


def preprocess_text(text):
    if isinstance(text, str):
        text = text.lower()
        text = text.translate(str.maketrans("", "", string.punctuation))
        words = word_tokenize(text)
        words = [word for word in words if word not in stopwords.words("english")]
        lemmatizer = WordNetLemmatizer()
        words = [lemmatizer.lemmatize(word) for word in words]
        return " ".join(words)
    return ""


df["Processed_Description"] = df["Description"].apply(preprocess_text)


print(df[["Category", "Processed_Description"]].head())

"""# Yeni Bölüm"""

Y=df['Category']
print(Y)
print(type(Y))
vectorizer = TfidfVectorizer()

X = vectorizer.fit_transform(df["Processed_Description"])


print(X.toarray())


print(vectorizer.get_feature_names_out())

from sklearn.svm import SVC
from sklearn.metrics import f1_score
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=42)
X_train=X_train[0:1000,:]
y_train=y_train[0:1000]
X_test=X_test[0:200,:]
y_test=y_test[0:200]
scaler=StandardScaler(with_mean=False)
X_train=scaler.fit_transform(X_train)
X_test=scaler.transform(X_test)

svm=SVC()
param_grid = {
    "C": [0.1,1,3,10],
    "kernel": ["linear","rbf","poly"],
    "gamma":["scale", "auto"]
}

# hyperparameter tuning
best_model, best_params, best_score,result_df = hyperparameter_tuning(svm, param_grid, X_train, y_train, search_type="grid")


print("Best Parameters:", best_params)
print("Best Score:", best_score)

import seaborn as sns
import matplotlib.pyplot as plt
import pandas as pd


filtered_df = result_df[result_df["param_gamma"] == "scale"]


heatmap_data = filtered_df.pivot(index="param_C", columns="param_kernel", values="mean_test_score")

plt.figure(figsize=(10, 6))
sns.heatmap(heatmap_data, annot=True, cmap="coolwarm", fmt=".3f", linewidths=0.5)

plt.xlabel("Kernel")
plt.ylabel("C (Regularization Parameter)")
plt.title("Heatmap of F1 Score vs. C and Kernel")
plt.yticks(rotation=0)
plt.show()



filtered_df = result_df[result_df["param_gamma"] == "auto"]


heatmap_data = filtered_df.pivot(index="param_C", columns="param_kernel", values="mean_test_score")

# Plot Heatmap
plt.figure(figsize=(10, 6))
sns.heatmap(heatmap_data, annot=True, cmap="coolwarm", fmt=".3f", linewidths=0.5)

plt.xlabel("Kernel")
plt.ylabel("C (Regularization Parameter)")
plt.title("Heatmap of F1 Score vs. C and Kernel")
plt.yticks(rotation=0)
plt.show()

predicts=best_model.predict(X_test)
print(f1_score(y_test,predicts,average='macro'))

pip install tensorflow

import pandas as pd
import numpy as np
import nltk
import string
import tensorflow as tf
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, Conv1D, GlobalMaxPooling1D, Dense, Dropout
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder

label_encoder = LabelEncoder()
df["Category"] = label_encoder.fit_transform(df["Category"])


tokenizer = Tokenizer(num_words=5000)
tokenizer.fit_on_texts(df["Processed_Description"])
sequences = tokenizer.texts_to_sequences(df["Processed_Description"])
word_index = tokenizer.word_index


max_length = 100
X = pad_sequences(sequences, maxlen=max_length)


Y = df["Category"].values

# Train-test split
X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=42)

# CNN model
model = Sequential([
    Embedding(input_dim=len(word_index) + 1, output_dim=100, input_length=max_length),
    Conv1D(filters=128, kernel_size=5, activation='relu'),
    GlobalMaxPooling1D(),
    Dense(64, activation='relu'),
    Dropout(0.5),
    Dense(len(set(Y)), activation='softmax')  # Output layer for multi-class classification
])

# Compile model
model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])

epochs = 3
train_accuracies = []
val_accuracies = []
train_losses = []
val_losses = []


for epoch in range(epochs):
    print(f"\nEpoch {epoch+1}/{epochs}:")
    model.fit(X_train, y_train, batch_size=32, verbose=1)

    # Evaluate on training and test data
    train_loss, train_acc = model.evaluate(X_train, y_train, verbose=0)
    val_loss, val_acc = model.evaluate(X_test, y_test, verbose=0)


    train_accuracies.append(train_acc * 100)
    val_accuracies.append(val_acc * 100)
    train_losses.append(train_loss)
    val_losses.append(val_loss)

    print(f"Train Accuracy: {train_acc * 100:.2f}%, Validation Accuracy: {val_acc * 100:.2f}%")

# Plot accuracy and loss curves
plt.figure(figsize=(12, 5))

# Accuracy plot
plt.subplot(1, 2, 1)
plt.plot(range(1, epochs + 1), train_accuracies, marker='o', linestyle='-', label="Train Accuracy")
plt.plot(range(1, epochs + 1), val_accuracies, marker='s', linestyle='--', label="Validation Accuracy")
plt.xlabel("Epochs")
plt.ylabel("Accuracy (%)")
plt.title("CNN Accuracy Over Epochs")
plt.legend()

# Loss plot
plt.subplot(1, 2, 2)
plt.plot(range(1, epochs + 1), train_losses, marker='o', linestyle='-', label="Train Loss")
plt.plot(range(1, epochs + 1), val_losses, marker='s', linestyle='--', label="Validation Loss")
plt.xlabel("Epochs")
plt.ylabel("Loss")
plt.title("CNN Loss Over Epochs")
plt.legend()


plt.show()
